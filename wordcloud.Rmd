---
title: "Wordcloud"
date: "`r Sys.Date()`"
output:
  html_document:
    df_print: paged
    toc: yes
  pdf_document:
    toc: yes
---
```{r setup, include = FALSE}
suppressWarnings(source("./fct/sources.R"))
suppressWarnings(library(tidyverse))
# knitr::opts_chunk$set(cache = TRUE)
require(devtools)
library(wordcloud2)
```


## accueil
```{r accueil, echo=FALSE}
text <- data.frame(accueil())
text <- text[2]

tm <- function(text){
  # Load the data as a corpus
  TextDoc <- Corpus(VectorSource(text))

  #Replacing "/", "@" and "|" with space
  toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
  removeAccentE <- content_transformer(function (x , pattern ) gsub(pattern, "e", x))
  removeAccentA <- content_transformer(function (x , pattern ) gsub(pattern, "a", x))
  removeC <- content_transformer(function (x , pattern ) gsub(pattern, "c", x))
  removeSpace <- content_transformer(function (x , pattern ) gsub(pattern, "", x))
  # TextDoc <- tm_map(TextDoc, toSpace, "/")
  TextDoc <- tm_map(TextDoc, toSpace, "@")
  TextDoc <- tm_map(TextDoc, toSpace, "\\|")
  TextDoc <- tm_map(TextDoc, toSpace, "'<'")
  TextDoc <- tm_map(TextDoc, toSpace, "\\'")
  TextDoc <- tm_map(TextDoc, removeAccentE, c("é","è","ê"))
  TextDoc <- tm_map(TextDoc, removeAccentA, c("à","â"))
  TextDoc <- tm_map(TextDoc, removeC, c("ç"))
  # Convert the text to lower case
  TextDoc <- tm_map(TextDoc, content_transformer(tolower))
  # Remove numbers
  # TextDoc <- tm_map(TextDoc, removeNumbers)
  # Remove english common stopwords
  # TextDoc <- tm_map(TextDoc, removeWords, stopwords("english"))
  # Remove your own stop word
  # specify your custom stopwords as a character vector
  TextDoc <- tm_map(TextDoc, removeWords, c("conclu", "conclubis", "eff","the"))
  # Remove punctuations
  TextDoc <- tm_map(TextDoc, removePunctuation)
  # Eliminate extra white spaces
  TextDoc <- tm_map(TextDoc, stripWhitespace)
  # Eliminate spaces
  # TextDoc <- gsub("[[:blank:]]", "", TextDoc)
  # Text stemming - which reduces words to their root form
  # TextDoc <- tm_map(TextDoc, stemDocument)

  # Build a term-document matrix
  TextDoc_dtm <- TermDocumentMatrix(TextDoc)
  dtm_m <- as.matrix(TextDoc_dtm)
  # Sort by descearing value of frequency
  dtm_v <- sort(rowSums(dtm_m),decreasing=TRUE)
  dtm_d <- data.frame(word = names(dtm_v),freq=dtm_v)
  # Display the top 20 most frequent words
  head(dtm_d, 30)

  # Plot the most frequent words
  barplot(dtm_d[1:20,]$freq, las = 2, names.arg = dtm_d[1:20,]$word,
          col =brewer.pal(8, "Dark2"), main ="Top 20 accueil",
          ylab = "Word frequencies")

  #generate word cloud
  # set.seed(1234)
  
  wordcloud(words = dtm_d$word, freq = dtm_d$freq, min.freq = 1,max.words=1500, random.order=FALSE, rot.per=0.35,colors=brewer.pal(8, "Dark2"))
  
  dev.print(device = png, file = "accueil_2.png", width = 600)
  
    wordcloud2(data = dtm_d, color = "random-light",backgroundColor ="black")
    
  
}

suppressWarnings(tm(text))
```

## en cours
```{r encours, echo=FALSE}
text <- data.frame(listDesc.ec())
text <- text[2]

tm <- function(text){
  # Load the data as a corpus
  TextDoc <- Corpus(VectorSource(text))

  #Replacing "/", "@" and "|" with space
  toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
  removeAccentE <- content_transformer(function (x , pattern ) gsub(pattern, "e", x))
  removeAccentA <- content_transformer(function (x , pattern ) gsub(pattern, "a", x))
  removeC <- content_transformer(function (x , pattern ) gsub(pattern, "c", x))
  removeSpace <- content_transformer(function (x , pattern ) gsub(pattern, "", x))
  # TextDoc <- tm_map(TextDoc, toSpace, "/")
  TextDoc <- tm_map(TextDoc, toSpace, "@")
  TextDoc <- tm_map(TextDoc, toSpace, "\\|")
  TextDoc <- tm_map(TextDoc, toSpace, "'<'")
  TextDoc <- tm_map(TextDoc, toSpace, "\\'")
  TextDoc <- tm_map(TextDoc, removeAccentE, c("é","è","ê"))
  TextDoc <- tm_map(TextDoc, removeAccentA, c("à","â"))
  TextDoc <- tm_map(TextDoc, removeC, c("ç"))
  # Convert the text to lower case
  TextDoc <- tm_map(TextDoc, content_transformer(tolower))
  # Remove numbers
  # TextDoc <- tm_map(TextDoc, removeNumbers)
  # Remove english common stopwords
  # TextDoc <- tm_map(TextDoc, removeWords, stopwords("english"))
  # Remove your own stop word
  # specify your custom stopwords as a character vector
  TextDoc <- tm_map(TextDoc, removeWords, c("conclu", "conclubis", "eff","the"))
  # Remove punctuations
  TextDoc <- tm_map(TextDoc, removePunctuation)
  # Eliminate extra white spaces
  TextDoc <- tm_map(TextDoc, stripWhitespace)
  # Eliminate spaces
  # TextDoc <- gsub("[[:blank:]]", "", TextDoc)
  # Text stemming - which reduces words to their root form
  # TextDoc <- tm_map(TextDoc, stemDocument)

  # Build a term-document matrix
  TextDoc_dtm <- TermDocumentMatrix(TextDoc)
  dtm_m <- as.matrix(TextDoc_dtm)
  # Sort by descearing value of frequency
  dtm_v <- sort(rowSums(dtm_m),decreasing=TRUE)
  dtm_d <- data.frame(word = names(dtm_v),freq=dtm_v)
  # Display the top 20 most frequent words
  head(dtm_d, 30)

  # Plot the most frequent words
  barplot(dtm_d[1:20,]$freq, las = 2, names.arg = dtm_d[1:20,]$word,
          col =brewer.pal(8, "Dark2"), main ="Top 20 en cours",
          ylab = "Word frequencies")

  #generate word cloud
  # set.seed(1234)
  
  wordcloud(words = dtm_d$word, freq = dtm_d$freq, min.freq = 1,max.words=1500, random.order=FALSE, rot.per=0.35,colors=brewer.pal(8, "Dark2"))
  
  dev.print(device = png, file = "encours_2.png", width = 600)
  
    # wordcloud2(data = dtm_d, color = "random-light",backgroundColor ="black")

  # 
}

suppressWarnings(tm(text))
```

## recent
```{r recent0, echo=FALSE}
recent <- data.frame(aAccueil())
text <- recent$Titre_1

tm <- function(text){
  # Load the data as a corpus
  TextDoc <- Corpus(VectorSource(text))

  #Replacing "/", "@" and "|" with space
  toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
  removeAccentE <- content_transformer(function (x , pattern ) gsub(pattern, "e", x))
  removeAccentA <- content_transformer(function (x , pattern ) gsub(pattern, "a", x))
  removeC <- content_transformer(function (x , pattern ) gsub(pattern, "c", x))
  removeSpace <- content_transformer(function (x , pattern ) gsub(pattern, "", x))
  # TextDoc <- tm_map(TextDoc, toSpace, "/")
  TextDoc <- tm_map(TextDoc, toSpace, "@")
  TextDoc <- tm_map(TextDoc, toSpace, "\\|")
  TextDoc <- tm_map(TextDoc, toSpace, "'<'")
  TextDoc <- tm_map(TextDoc, toSpace, "\\'")
  TextDoc <- tm_map(TextDoc, removeAccentE, c("é","è","ê"))
  TextDoc <- tm_map(TextDoc, removeAccentA, c("à","â"))
  TextDoc <- tm_map(TextDoc, removeC, c("ç"))
  # Convert the text to lower case
  TextDoc <- tm_map(TextDoc, content_transformer(tolower))
  # Remove numbers
  # TextDoc <- tm_map(TextDoc, removeNumbers)
  # Remove english common stopwords
  # TextDoc <- tm_map(TextDoc, removeWords, stopwords("english"))
  # Remove your own stop word
  # specify your custom stopwords as a character vector
  TextDoc <- tm_map(TextDoc, removeWords, c("conclu", "conclubis", "eff","the","titre"))
  # Remove punctuations
  TextDoc <- tm_map(TextDoc, removePunctuation)
  # Eliminate extra white spaces
  TextDoc <- tm_map(TextDoc, stripWhitespace)
  # Eliminate spaces
  # TextDoc <- gsub("[[:blank:]]", "", TextDoc)
  # Text stemming - which reduces words to their root form
  # TextDoc <- tm_map(TextDoc, stemDocument)

  # Build a term-document matrix
  TextDoc_dtm <- TermDocumentMatrix(TextDoc)
  dtm_m <- as.matrix(TextDoc_dtm)
  # Sort by descearing value of frequency
  dtm_v <- sort(rowSums(dtm_m),decreasing=TRUE)
  dtm_d <- data.frame(word = names(dtm_v),freq=dtm_v)
  # Display the top 20 most frequent words
  head(dtm_d, 30)

  # Plot the most frequent words
  barplot(dtm_d[1:20,]$freq, las = 2, names.arg = dtm_d[1:20,]$word,
          col =brewer.pal(8, "Dark2"), main ="Top 20 recent",
          ylab = "Word frequencies")

  #generate word cloud
  # set.seed(1234)
  
  wordcloud(words = dtm_d$word, freq = dtm_d$freq, min.freq = 1,max.words=1500, random.order=FALSE, rot.per=0.35,colors=brewer.pal(8, "Dark2"))
  
  # dev.print(device = png, file = "recent_2.png", width = 600)
  
    # wordcloud2(data = dtm_d, color = "random-light",backgroundColor ="black")

  # 
}

suppressWarnings(tm(text))
```

## termine
```{r termine, echo=FALSE}
text <- data.frame(listDesc.ter())
text <- text[2]

tm <- function(text){
  # Load the data as a corpus
  TextDoc <- Corpus(VectorSource(text))

  #Replacing "/", "@" and "|" with space
  toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
  removeAccentE <- content_transformer(function (x , pattern ) gsub(pattern, "e", x))
  removeAccentA <- content_transformer(function (x , pattern ) gsub(pattern, "a", x))
  removeC <- content_transformer(function (x , pattern ) gsub(pattern, "c", x))
  removeSpace <- content_transformer(function (x , pattern ) gsub(pattern, "", x))
  # TextDoc <- tm_map(TextDoc, toSpace, "/")
  TextDoc <- tm_map(TextDoc, toSpace, "@")
  TextDoc <- tm_map(TextDoc, toSpace, "\\|")
  TextDoc <- tm_map(TextDoc, toSpace, "'<'")
  TextDoc <- tm_map(TextDoc, toSpace, "\\'")
  # TextDoc <- tm_map(TextDoc, removeAccentE, c("é","è","ê"))
  # TextDoc <- tm_map(TextDoc, removeAccentA, c("à","â"))
  # TextDoc <- tm_map(TextDoc, removeC, c("ç"))
  # Convert the text to lower case
  TextDoc <- tm_map(TextDoc, content_transformer(tolower))
  # Remove numbers
  # TextDoc <- tm_map(TextDoc, removeNumbers)
  # Remove english common stopwords
  # TextDoc <- tm_map(TextDoc, removeWords, stopwords("english"))
  # Remove your own stop word
  # specify your custom stopwords as a character vector
  TextDoc <- tm_map(TextDoc, removeWords, c("conclu", "conclubis", "eff","the"))
  # Remove punctuations
  TextDoc <- tm_map(TextDoc, removePunctuation)
  # Eliminate extra white spaces
  TextDoc <- tm_map(TextDoc, stripWhitespace)
  # Eliminate spaces
  # TextDoc <- gsub("[[:blank:]]", "", TextDoc)
  # Text stemming - which reduces words to their root form
  # TextDoc <- tm_map(TextDoc, stemDocument)

  # Build a term-document matrix
  TextDoc_dtm <- TermDocumentMatrix(TextDoc)
  dtm_m <- as.matrix(TextDoc_dtm)
  # Sort by descearing value of frequency
  dtm_v <- sort(rowSums(dtm_m),decreasing=TRUE)
  dtm_d <- data.frame(word = names(dtm_v),freq=dtm_v)
  # Display the top 20 most frequent words
  head(dtm_d, 30)

  # Plot the most frequent words
  barplot(dtm_d[1:20,]$freq, las = 2, names.arg = dtm_d[1:20,]$word,
          col =brewer.pal(8, "Dark2"), main ="Top 20 termine",
          ylab = "Word frequencies")

  #generate word cloud
  # set.seed(1234)
  
  wordcloud(words = dtm_d$word, freq = dtm_d$freq, min.freq = 1,max.words=1500, random.order=FALSE, rot.per=0.35,colors=brewer.pal(8, "Dark2"))
  
  dev.print(device = png, file = "termine_2.png", width = 600)
  
    # wordcloud2(data = dtm_d, color = "random-light",backgroundColor ="black")

  # 
}

suppressWarnings(tm(text))
```

## copy
```{r copy, echo=FALSE}

main <- function(){
  text <- data.frame(copy())
  # Read the text file from local machine , choose file interactively
  # text2 <- readLines(file.choose())
  # Load the data as a corpus
  TextDoc <- Corpus(VectorSource(text))

  #Replacing "/", "@" and "|" with space
  toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
  removeSpace <- content_transformer(function (x , pattern ) gsub(pattern, "", x))
  TextDoc <- tm_map(TextDoc, toSpace, "/")
  TextDoc <- tm_map(TextDoc, toSpace, "@")
  TextDoc <- tm_map(TextDoc, toSpace, "\\|")
  # Convert the text to lower case
  TextDoc <- tm_map(TextDoc, content_transformer(tolower))
  # Remove numbers
  # TextDoc <- tm_map(TextDoc, removeNumbers)
  # Remove english common stopwords
  # TextDoc <- tm_map(TextDoc, removeWords, stopwords("english"))
  # Remove your own stop word
  # specify your custom stopwords as a character vector
  TextDoc <- tm_map(TextDoc, removeWords, c("conclu", "conclubis", "eff","the"))
  # Remove punctuations
  TextDoc <- tm_map(TextDoc, removePunctuation)
  # Eliminate extra white spaces
  TextDoc <- tm_map(TextDoc, stripWhitespace)
  # Eliminate spaces
  # TextDoc <- gsub("[[:blank:]]", "", TextDoc)
  # Text stemming - which reduces words to their root form
  # TextDoc <- tm_map(TextDoc, stemDocument)

  # Build a term-document matrix
  TextDoc_dtm <- TermDocumentMatrix(TextDoc)
  dtm_m <- as.matrix(TextDoc_dtm)
  # Sort by descearing value of frequency
  dtm_v <- sort(rowSums(dtm_m),decreasing=TRUE)
  dtm_d <- data.frame(word = names(dtm_v) ,freq=dtm_v)
  # Display the top 20 most frequent words
  head(dtm_d, 30)

  # Plot the most frequent words
  barplot(dtm_d[1:20,]$freq, las = 2, names.arg = dtm_d[1:20,]$word,
          col =brewer.pal(8, "Dark2"), main ="Top 20 copy",
          ylab = "Word frequencies")

  #generate word cloud
  # set.seed(1234)
  wordcloud(words = dtm_d$word, freq = dtm_d$freq, min.freq = 1,max.words=1500, random.order=FALSE, rot.per=0.35,colors=brewer.pal(8, "Dark2"))
  
  dev.print(device = png, file = "copy_2.png", width=600)

    # wordcloud2(data = dtm_d, color = "random-light",backgroundColor ="black")

}

suppressWarnings(main())
```

## conclu
```{r conclu, echo=FALSE}

main <- function(){
  text2 <- data.frame(aTestConclu())
  text2 <- text2[2]
  # Read the text file from local machine , choose file interactively
  # text2 <- readLines(file.choose())
  # Load the data as a corpus
  TextDoc <- Corpus(VectorSource(text2))

  #Replacing "/", "@" and "|" with space
  toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
  removeSpace <- content_transformer(function (x , pattern ) gsub(pattern, "", x))
  TextDoc <- tm_map(TextDoc, toSpace, "/")
  TextDoc <- tm_map(TextDoc, toSpace, "@")
  TextDoc <- tm_map(TextDoc, toSpace, "\\|")
  # Convert the text to lower case
  TextDoc <- tm_map(TextDoc, content_transformer(tolower))
  # Remove numbers
  # TextDoc <- tm_map(TextDoc, removeNumbers)
  # Remove english common stopwords
  # TextDoc <- tm_map(TextDoc, removeWords, stopwords("english"))
  # Remove your own stop word
  # specify your custom stopwords as a character vector
  TextDoc <- tm_map(TextDoc, removeWords, c("conclu", "conclubis","the", "eff", "na", "conclucompi", "conclucompibis"))
  # Remove punctuations
  TextDoc <- tm_map(TextDoc, removePunctuation)
  # Eliminate extra white spaces
  TextDoc <- tm_map(TextDoc, stripWhitespace)
  # Eliminate spaces
  # TextDoc <- gsub("[[:blank:]]", "", TextDoc)
  # Text stemming - which reduces words to their root form
  # TextDoc <- tm_map(TextDoc, stemDocument)

  # Build a term-document matrix
  TextDoc_dtm <- TermDocumentMatrix(TextDoc)
  dtm_m <- as.matrix(TextDoc_dtm)
  # Sort by descearing value of frequency
  dtm_v <- sort(rowSums(dtm_m),decreasing=TRUE)
  dtm_d <- data.frame(word = names(dtm_v),freq=dtm_v)
  # Display the top 20 most frequent words
  head(dtm_d, 30)

  # Plot the most frequent words
  barplot(dtm_d[1:20,]$freq, las = 2, names.arg = dtm_d[1:20,]$word,
          col =brewer.pal(8, "Dark2"), main ="Top 20 conclu",
          ylab = "Word frequencies")

  #generate word cloud
  # set.seed(1234)
  wordcloud(words = dtm_d$word, freq = dtm_d$freq, min.freq = 1,max.words=1500, random.order=FALSE, rot.per=0.35,colors=brewer.pal(8, "Dark2"))
  
  dev.print(device = png, file = "conclu_2.png", width = 600)
  
    # wordcloud2(data = dtm_d, color = "random-light",backgroundColor ="black")

  

}

suppressWarnings(main())
```

## conclubis
```{r conclubis, echo=FALSE}

main <- function(){
  text2 <- data.frame(aTestConcluBis())
  text2 <- text2[2]
  # Read the text file from local machine , choose file interactively
  # text2 <- readLines(file.choose())
  # Load the data as a corpus
  TextDoc <- Corpus(VectorSource(text2))

  #Replacing "/", "@" and "|" with space
  toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
  removeSpace <- content_transformer(function (x , pattern ) gsub(pattern, "", x))
  TextDoc <- tm_map(TextDoc, toSpace, "/")
  TextDoc <- tm_map(TextDoc, toSpace, "@")
  TextDoc <- tm_map(TextDoc, toSpace, "\\|")
  # Convert the text to lower case
  TextDoc <- tm_map(TextDoc, content_transformer(tolower))
  # Remove numbers
  # TextDoc <- tm_map(TextDoc, removeNumbers)
  # Remove english common stopwords
  # TextDoc <- tm_map(TextDoc, removeWords, stopwords("english"))
  # Remove your own stop word
  # specify your custom stopwords as a character vector
  TextDoc <- tm_map(TextDoc, removeWords, c("conclu", "conclubis","the", "eff", "na", "conclucompi", "conclucompibis"))
  # Remove punctuations
  TextDoc <- tm_map(TextDoc, removePunctuation)
  # Eliminate extra white spaces
  TextDoc <- tm_map(TextDoc, stripWhitespace)
  # Eliminate spaces
  # TextDoc <- gsub("[[:blank:]]", "", TextDoc)
  # Text stemming - which reduces words to their root form
  # TextDoc <- tm_map(TextDoc, stemDocument)

  # Build a term-document matrix
  TextDoc_dtm <- TermDocumentMatrix(TextDoc)
  dtm_m <- as.matrix(TextDoc_dtm)
  # Sort by descearing value of frequency
  dtm_v <- sort(rowSums(dtm_m),decreasing=TRUE)
  dtm_d <- data.frame(word = names(dtm_v),freq=dtm_v)
  # Display the top 20 most frequent words
  head(dtm_d, 30)

  # Plot the most frequent words
  barplot(dtm_d[1:20,]$freq, las = 2, names.arg = dtm_d[1:20,]$word,
          col =brewer.pal(8, "Dark2"), main ="Top 20 conclubis",
          ylab = "Word frequencies")

  #generate word cloud
  # set.seed(1234)
  wordcloud(words = dtm_d$word, freq = dtm_d$freq, min.freq = 1,max.words=1500, random.order=FALSE, rot.per=0.35,colors=brewer.pal(8, "Dark2"))
  dev.print(device = png, file = "conclubis_2.png", width = 600)
  
    # wordcloud2(data = dtm_d, color = "random-light",backgroundColor ="black")

  

}

suppressWarnings(main())
```

## sem
```{r sem, echo=FALSE}

main <- function(){
  text2 <- data.frame(copy.sem())
  text2 <- text2[2]
  # Read the text file from local machine , choose file interactively
  # text2 <- readLines(file.choose())
  # Load the data as a corpus
  TextDoc <- Corpus(VectorSource(text2))

  #Replacing "/", "@" and "|" with space
  toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
  removeSpace <- content_transformer(function (x , pattern ) gsub(pattern, "", x))
  TextDoc <- tm_map(TextDoc, toSpace, "/")
  TextDoc <- tm_map(TextDoc, toSpace, "@")
  TextDoc <- tm_map(TextDoc, toSpace, "\\|")
  # Convert the text to lower case
  TextDoc <- tm_map(TextDoc, content_transformer(tolower))
  # Remove numbers
  TextDoc <- tm_map(TextDoc, removeNumbers)
  # Remove english common stopwords
  # TextDoc <- tm_map(TextDoc, removeWords, stopwords("english"))
  # Remove your own stop word
  # specify your custom stopwords as a character vector
  TextDoc <- tm_map(TextDoc, removeWords, c("conclu", "conclubis","the", "eff", "na", "conclucompi", "conclucompibis","sem"))
  # Remove punctuations
  TextDoc <- tm_map(TextDoc, removePunctuation)
  # Eliminate extra white spaces
  TextDoc <- tm_map(TextDoc, stripWhitespace)
  # Eliminate spaces
  # TextDoc <- gsub("[[:blank:]]", "", TextDoc)
  # Text stemming - which reduces words to their root form
  # TextDoc <- tm_map(TextDoc, stemDocument)

  # Build a term-document matrix
  TextDoc_dtm <- TermDocumentMatrix(TextDoc)
  dtm_m <- as.matrix(TextDoc_dtm)
  # Sort by descearing value of frequency
  dtm_v <- sort(rowSums(dtm_m),decreasing=TRUE)
  dtm_d <- data.frame(word = names(dtm_v),freq=dtm_v)
  # Display the top 20 most frequent words
  head(dtm_d, 30)

  # Plot the most frequent words
  barplot(dtm_d[1:20,]$freq, las = 2, names.arg = dtm_d[1:20,]$word,
          col =brewer.pal(8, "Dark2"), main ="Top 20 sem",
          ylab = "Word frequencies")

  #generate word cloud
  # set.seed(1234)
  wordcloud(words = dtm_d$word, freq = dtm_d$freq, min.freq = 1,max.words=1500, random.order=FALSE, rot.per=0.35,colors=brewer.pal(8, "Dark2"))
dev.print(device = png, file = "sem_2.png", width = 600)

      # wordcloud2(data = dtm_d, color = "random-light",backgroundColor ="black")

  

}

suppressWarnings(main())
```

## mois
```{r mois, echo=FALSE}

main <- function(){
  text2 <- data.frame(copy.mois())
  text2 <- text2[2]
  # Read the text file from local machine , choose file interactively
  # text2 <- readLines(file.choose())
  # Load the data as a corpus
  TextDoc <- Corpus(VectorSource(text2))

  #Replacing "/", "@" and "|" with space
  toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
  removeSpace <- content_transformer(function (x , pattern ) gsub(pattern, "", x))
  TextDoc <- tm_map(TextDoc, toSpace, "/")
  TextDoc <- tm_map(TextDoc, toSpace, "@")
  TextDoc <- tm_map(TextDoc, toSpace, "\\|")
  # Convert the text to lower case
  TextDoc <- tm_map(TextDoc, content_transformer(tolower))
  # Remove numbers
  TextDoc <- tm_map(TextDoc, removeNumbers)
  # Remove english common stopwords
  # TextDoc <- tm_map(TextDoc, removeWords, stopwords("english"))
  # Remove your own stop word
  # specify your custom stopwords as a character vector
  TextDoc <- tm_map(TextDoc, removeWords, c("conclu", "conclubis","the", "eff", "na", "conclucompi", "conclucompibis","mois"))
  # Remove punctuations
  TextDoc <- tm_map(TextDoc, removePunctuation)
  # Eliminate extra white spaces
  TextDoc <- tm_map(TextDoc, stripWhitespace)
  # Eliminate spaces
  # TextDoc <- gsub("[[:blank:]]", "", TextDoc)
  # Text stemming - which reduces words to their root form
  # TextDoc <- tm_map(TextDoc, stemDocument)

  # Build a term-document matrix
  TextDoc_dtm <- TermDocumentMatrix(TextDoc)
  dtm_m <- as.matrix(TextDoc_dtm)
  # Sort by descearing value of frequency
  dtm_v <- sort(rowSums(dtm_m),decreasing=TRUE)
  dtm_d <- data.frame(word = names(dtm_v),freq=dtm_v)
  # Display the top 20 most frequent words
  head(dtm_d, 30)

  # Plot the most frequent words
  barplot(dtm_d[1:20,]$freq, las = 2, names.arg = dtm_d[1:20,]$word,
          col =brewer.pal(8, "Dark2"), main ="Top 20 mois",
          ylab = "Word frequencies")

  #generate word cloud
  # set.seed(1234)
  wordcloud(words = dtm_d$word, freq = dtm_d$freq, min.freq = 1,max.words=1500, random.order=FALSE, rot.per=0.35,colors=brewer.pal(8, "Dark2"))
  dev.print(device = png, file = "mois_2.png", width=600)
  
    # wordcloud2(data = dtm_d, color = "random-light",shape="circle",backgroundColor ="black")

  # 

}

suppressWarnings(main())
```

## max
```{r max, echo=FALSE}

main <- function(){
  text2 <- copy.max()
  text2 <- text2[2]
  # Read the text file from local machine , choose file interactively
  # text2 <- readLines(file.choose())
  # Load the data as a corpus
  TextDoc <- Corpus(VectorSource(text2))

  #Replacing "/", "@" and "|" with space
  toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
  removeSpace <- content_transformer(function (x , pattern ) gsub(pattern, "", x))
  TextDoc <- tm_map(TextDoc, toSpace, "/")
  TextDoc <- tm_map(TextDoc, toSpace, "@")
  TextDoc <- tm_map(TextDoc, toSpace, "\\|")
  # Convert the text to lower case
  TextDoc <- tm_map(TextDoc, content_transformer(tolower))
  # Remove numbers
  TextDoc <- tm_map(TextDoc, removeNumbers)
  # Remove english common stopwords
  # TextDoc <- tm_map(TextDoc, removeWords, stopwords("english"))
  # Remove your own stop word
  # specify your custom stopwords as a character vector
  TextDoc <- tm_map(TextDoc, removeWords, c("conclu", "conclubis", "eff","na","the","conclucompi","conclucompibis"))
  # Remove punctuations
  TextDoc <- tm_map(TextDoc, removePunctuation)
  # Eliminate extra white spaces
  TextDoc <- tm_map(TextDoc, stripWhitespace)
  # Eliminate spaces
  # TextDoc <- gsub("[[:blank:]]", "", TextDoc)
  # Text stemming - which reduces words to their root form
  # TextDoc <- tm_map(TextDoc, stemDocument)

  # Build a term-document matrix
  TextDoc_dtm <- TermDocumentMatrix(TextDoc)
  dtm_m <- as.matrix(TextDoc_dtm)
  # Sort by descearing value of frequency
  dtm_v <- sort(rowSums(dtm_m),decreasing=TRUE)
  dtm_d <- data.frame(word = names(dtm_v),freq=dtm_v)
  # Display the top 20 most frequent words
  head(dtm_d, 30)

  # Plot the most frequent words
  barplot(
    dtm_d[1:20,]$freq, 
    las = 2, 
    names.arg = dtm_d[1:20,]$word,
    col =brewer.pal(8, "Dark2"), 
    main ="Top 20 max",
    ylab = "Word frequencies")

  #generate word cloud
  # set.seed(1234)
  wordcloud(words = dtm_d$word, freq = dtm_d$freq, min.freq = 1,max.words=1500, random.order=FALSE, rot.per=0.35,colors=brewer.pal(8, "Dark2"))
  dev.print(device = png, file = "max_2.png", width = 600)
    # wordcloud2(data = dtm_d, color = "random-light",backgroundColor ="black")

  

}

suppressWarnings(main())
```

## eff
```{r eff, echo=FALSE}

main <- function(){
  text2 <- aEff()
  text2 <- text2[2]
  # Read the text file from local machine , choose file interactively
  # text2 <- readLines(file.choose())
  # Load the data as a corpus
  TextDoc <- Corpus(VectorSource(text2))

  #Replacing "/", "@" and "|" with space
  toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
  removeSpace <- content_transformer(function (x , pattern ) gsub(pattern, "", x))
  TextDoc <- tm_map(TextDoc, toSpace, "/")
  TextDoc <- tm_map(TextDoc, toSpace, "@")
  TextDoc <- tm_map(TextDoc, toSpace, "\\|")
  # Convert the text to lower case
  TextDoc <- tm_map(TextDoc, content_transformer(tolower))
  # Remove numbers
  TextDoc <- tm_map(TextDoc, removeNumbers)
  # Remove english common stopwords
  # TextDoc <- tm_map(TextDoc, removeWords, stopwords("english"))
  # Remove your own stop word
  # specify your custom stopwords as a character vector
  TextDoc <- tm_map(TextDoc, removeWords, c("conclu", "conclubis", "eff","na","the","conclucompi","conclucompibis"))
  # Remove punctuations
  TextDoc <- tm_map(TextDoc, removePunctuation)
  # Eliminate extra white spaces
  TextDoc <- tm_map(TextDoc, stripWhitespace)
  # Eliminate spaces
  # TextDoc <- gsub("[[:blank:]]", "", TextDoc)
  # Text stemming - which reduces words to their root form
  # TextDoc <- tm_map(TextDoc, stemDocument)

  # Build a term-document matrix
  TextDoc_dtm <- TermDocumentMatrix(TextDoc)
  dtm_m <- as.matrix(TextDoc_dtm)
  # Sort by descearing value of frequency
  dtm_v <- sort(rowSums(dtm_m),decreasing=TRUE)
  dtm_d <- data.frame(word = names(dtm_v),freq=dtm_v)
  # Display the top 20 most frequent words
  head(dtm_d, 30)

  # Plot the most frequent words
  barplot(
    dtm_d[1:20,]$freq, 
    las = 2, 
    names.arg = dtm_d[1:20,]$word,
    col =brewer.pal(8, "Dark2"), 
    main ="Top 20 eff",
    ylab = "Word frequencies")

  #generate word cloud
  # set.seed(1234)
  wordcloud(words = dtm_d$word, freq = dtm_d$freq, min.freq = 1,max.words=1500, random.order=FALSE, rot.per=0.35,colors=brewer.pal(8, "Dark2"))
  dev.print(device = png, file = "eff_2.png", width = 600)
  
    # wordcloud2(data = dtm_d, color = "random-light",backgroundColor ="black")

  # 

}

suppressWarnings(main())
```

## resume
```{r resume, echo=FALSE}

main <- function(){
  text2 <- data.frame(aTestResume())
  text2 <- text2[2]
  # Read the text file from local machine , choose file interactively
  # text2 <- readLines(file.choose())
  # Load the data as a corpus
  TextDoc <- Corpus(VectorSource(text2))

  #Replacing "/", "@" and "|" with space
  toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
  removeSpace <- content_transformer(function (x , pattern ) gsub(pattern, "", x))
  TextDoc <- tm_map(TextDoc, toSpace, "/")
  TextDoc <- tm_map(TextDoc, toSpace, "@")
  TextDoc <- tm_map(TextDoc, toSpace, "\\|")
  # Convert the text to lower case
  TextDoc <- tm_map(TextDoc, content_transformer(tolower))
  # Remove numbers
  TextDoc <- tm_map(TextDoc, removeNumbers)
  # Remove english common stopwords
  # TextDoc <- tm_map(TextDoc, removeWords, stopwords("english"))
  # Remove your own stop word
  # specify your custom stopwords as a character vector
  TextDoc <- tm_map(TextDoc, removeWords, c("conclu", "conclubis","the", "eff","na","conclucompi","conclucompibis"))
  # Remove punctuations
  TextDoc <- tm_map(TextDoc, removePunctuation)
  # Eliminate extra white spaces
  TextDoc <- tm_map(TextDoc, stripWhitespace)
  # Eliminate spaces
  # TextDoc <- gsub("[[:blank:]]", "", TextDoc)
  # Text stemming - which reduces words to their root form
  # TextDoc <- tm_map(TextDoc, stemDocument)

  # Build a term-document matrix
  TextDoc_dtm <- TermDocumentMatrix(TextDoc)
  dtm_m <- as.matrix(TextDoc_dtm)
  # Sort by descearing value of frequency
  dtm_v <- sort(rowSums(dtm_m),decreasing=TRUE)
  dtm_d <- data.frame(word = names(dtm_v),freq=dtm_v)
  # Display the top 20 most frequent words
  head(dtm_d, 30)

  # Plot the most frequent words
  barplot(dtm_d[1:20,]$freq, las = 2, names.arg = dtm_d[1:20,]$word,
          col =brewer.pal(8, "Dark2"), main ="Top 20 resume",
          ylab = "Word frequencies")

  #generate word cloud
  # set.seed(1234)
  wordcloud(words = dtm_d$word, freq = dtm_d$freq, min.freq = 1,max.words=1500, random.order=FALSE, rot.per=0.35,colors=brewer.pal(8, "Dark2"))
  dev.print(device = png, file = "resume_2.png", width = 600)
  
    # wordcloud2(data = dtm_d, color = "random-light",backgroundColor ="black")

  # 

}

suppressWarnings(main())
```


